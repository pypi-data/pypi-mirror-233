{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQjDIrksKCeP"
   },
   "source": [
    "# [config, imports]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6CddP8FWNe3"
   },
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "P_WP8IB5Ji71"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import os, sys\n",
    "import typing as T\n",
    "\n",
    "import os, sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wXH2vShda3vL"
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_ORGANIZATION\"] = \"org-k3sf2XUmt2A7lsAXjXgOz3DQ\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hgIoBfT5mF50"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo '{\n",
    "  \"type\": \"service_account\",\n",
    "  \"project_id\": \"chatonomicon\",\n",
    "  \"private_key_id\": \"6ec09fb5e6719538fa7294eaf8a42d4d2f087927\",\n",
    "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDJIS5vDRVsXIWk\\nE0s0gfUl1UjTwmjfiTae8M3oK9Z+f2a/Iyu8RLPGnacq8mty6LNkaNdIFiBHb3IV\\nBVX9G4EZEOt/A/cZyezfA6YpKqXMZCz8VHXuOFune2UE47IhgoWukFxRr2P83w1f\\nvjLGBDQE67K3LUT+3/V/MzCfNX9U2Uv/R493fHYuJy84Wa51E+qVccbP+I97Lp66\\nuWhn55SDA+DsiayR4FR3Ojy6zFpZzPiPluLjTiu+XOBkvmSFDBnKFwA5gOv/mPP1\\nF8eQFyQx+D84mWiVN3zPCVy5IBR7PVy6/BqQL2E/Y5MU/kqchlEQ8qqrhZYNa3Xf\\nxvhtgHepAgMBAAECggEAFj21N+pJWM/j5tyL09D9qgr7eFA+nt56Y+nSyTZkzBz8\\nQI1r+hAaVa+sE1GWnHC7WrKFwObSd7fNFKJAC6LRnDMc6ZEnj2pfB1vM0W9SAYe/\\n2kDgQCErEHqSlY90zKQVcpPr44wKuWU0gkaC8Jb5JiSmwP8QasNbD096tFZa/+er\\nmOT2+Emvo+T26jLrJD4EljtYvecPMU6ATBNeAfApODx/00GFXs4yIzlS2gCyKI/c\\nbyYILv7/Xw+SbEpVfKgY1NgPOYxnhfQvYqaP+Pdg8zWAsXFgZGwD7sifkldgMHgW\\nhhr35tTZZuJP0SO8OyY85daTVnnyg3fJCe6ydOkdsQKBgQD8idEf2UHMp7ltTsZ6\\nR6m5s7JClVSKN1HncZPZss4brcSXE6hC+qcL/9bO1N9A8GBDMqOxcrpe/9m9W3Wl\\n8Y3UH14aj1eoAL2Kc1rW1frpbg1TpFeGgDu+K1l5QxpsuAPv3Pjr9Gtaq1Fm7ETw\\nIN7W8HpP3DHrsO7aJpUfUD4DEQKBgQDL4vdILsneowiJsT0SLk0V1i1k02PmKq6r\\n4dAQrnGYpW2PqmPozwVJooi3RL+QnRKUGJXWN9FlmpXA3Iutv7QAOulJ/+yVrMPG\\n6QGX4X0qvPpkLsyHOLsAElWOVuPTplY8p+nYbbPmlX57rQiyTs+gPTddbSwljtXh\\nq9Mv6ml7GQKBgGKtV1Giitt1Djdv+I3/QoqVdKofv5DiXNaawrLl8zNNiuudernx\\nRyShK0ZWV4YAZxzaxKupsLk+L86V1jqpUEn6l8K5D/9NisJ2oKAhrJFZt0tfg/PC\\nV5XPSn7fgYvJu4AWUA2iy+/50SzuVWGe9nP2M1TRLZBy6mHDgbFioViBAoGAHASO\\nkr7LQusIi1Nt3we51BxglJBwE/sdkcUwQHqYZa+mBK4XmLCKO3o4a2bpJEhRe0R9\\norh88Ad0ONKV50SGydlOobqnmlHpfXYbn/F+r9bUWdwMgzCKkhHy6+KI4FblVuSE\\nQBejNzan8PrwTQCDi7Od3hrYjDi6r45bMSR3ZlECgYEAit6POyks5Q2I3vQbnR9I\\nV8fw2gwRZLH81kIoEpoiL95KuV+DiyEJNECgEQ5Vs9xbD50ChSEC6iti4c17ap8X\\n3G4+XE8CZlO+f5BzK1TwgnXVoT+OR5DjJeUM1DlPKGky1OqQ43XQjEgmY+lyCwWJ\\nNjQ2mU2TNLQbNwpYYk2IrKY=\\n-----END PRIVATE KEY-----\\n\",\n",
    "  \"client_email\": \"palm-test@chatonomicon.iam.gserviceaccount.com\",\n",
    "  \"client_id\": \"112939691223070442732\",\n",
    "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/palm-test%40chatonomicon.iam.gserviceaccount.com\",\n",
    "  \"universe_domain\": \"googleapis.com\"\n",
    "}' > ./tmp.sak.chatonomicon-6ec09fb5e671.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ax9l_0SwmgRX"
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = './tmp.sak.chatonomicon-6ec09fb5e671.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0a6U73yLa0wi"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display as dy, display_markdown\n",
    "\n",
    "dmd = partial(display_markdown, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BgfVEDNaeey"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBUURi7heFfO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YjZPuFuJ3kc"
   },
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "_xxzo--ULkuK"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Message:\n",
    "    # eg. OpenAI \"role\" (\"system\", \"user\" or \"assistant\")\n",
    "    # or Vertex/Google \"author\" (\"user\" or \"bot\")\n",
    "    author: T.Literal[\"user\", \"bot\"]\n",
    "    content: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Conversation:\n",
    "    messages: list[Message]\n",
    "    # eg. OpenAI \"system prompt\" or Vertex/Google \"context\"\n",
    "    context_prompt: str | None = None\n",
    "    # list of (\"input\", \"output\") pairs\n",
    "    examples: list[tuple[str, str]] | None = None\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    conversation: Conversation = None\n",
    "\n",
    "    def ask(\n",
    "        self, text: str, *, skip_history=False, **params\n",
    "    ) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def clone(self) -> \"Agent\":\n",
    "        return deepcopy(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AgentNbUI:\n",
    "    agent: Agent\n",
    "\n",
    "    def ask(\n",
    "        self, text: str, *, skip_history=False, display=True, **params\n",
    "    ) -> str | None:\n",
    "        out = self.agent.ask(text, skip_history=skip_history, **params)\n",
    "        if display:\n",
    "            dmd(out)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def show_conversation(self) -> None:\n",
    "        c = self.agent.conversation\n",
    "        out = \"\"\n",
    "        if c.context_prompt:\n",
    "            out += \"**Context/System prompt:** \" + c.context_prompt\n",
    "            out += \"\\n\\n---\\n\"\n",
    "        if c.examples:\n",
    "            out += \"**Examples:**\\n\"\n",
    "            for e in c.examples:\n",
    "                out += \"\\n**&rarr; Input:** \" + e[0]\n",
    "                out += \"\\n**Output:** \" + e[0]\n",
    "            out += \"\\n\\n---\\n\"\n",
    "        for i, m in enumerate(c.messages or ()):\n",
    "            out += f\"**{m.author.capitalize()}[{i // 2}]:** {m.content}\\n\\n\"\n",
    "            if m.author != \"user\":\n",
    "                out += \"\\n\\n---\\n\"\n",
    "        dmd(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "0PNhcDTQljCV",
    "outputId": "109406aa-5784-49b7-9740-10949e8c32ac"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but as an AI language model, I don't have real-time information or the capability to observe the current situation. If blood is dripping from the sky in real life, it would be highly unusual and possibly indicate a dangerous or extraordinary event that requires immediate attention from relevant authorities. It is recommended to alert local emergency services or authorities in such cases."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "class OpenAIAgent(Agent):\n",
    "    conversation: Conversation\n",
    "    params: dict\n",
    "\n",
    "    def __init__(self, conversation, **params):\n",
    "        self.conversation = conversation\n",
    "        self.params = params\n",
    "\n",
    "    def ask(\n",
    "        self, text: str, *, skip_history=False, display=True, **params\n",
    "    ) -> str:\n",
    "        new_messages = deepcopy(self.conversation.messages)\n",
    "        new_messages.append(Message(\"user\", text))\n",
    "\n",
    "        create_args = {\n",
    "            **self.params,\n",
    "            **params,\n",
    "            \"messages\": self._make_api_messages(new_messages)\n",
    "        }\n",
    "        self.last_res_ = openai.ChatCompletion.create(**create_args)\n",
    "\n",
    "        out = self.last_res_.choices[0].message.content\n",
    "        assert out is not None\n",
    "\n",
    "        if not skip_history:\n",
    "            new_messages.append(Message(\"bot\", out))\n",
    "            self.conversation.messages = new_messages\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _make_api_messages(self, messages: list[Message]) -> list[dict]:\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"assistant\" if m.author == \"bot\" else m.author,\n",
    "                \"content\": m.content\n",
    "            }\n",
    "            for m in messages\n",
    "        ]\n",
    "\n",
    "\n",
    "conv = Conversation(\n",
    "    context_prompt=\"You are a sarcastic but helpful assistant.\",\n",
    "    messages=[]\n",
    ")\n",
    "agent10ui = AgentNbUI( OpenAIAgent(deepcopy(conv), model=\"gpt-3.5-turbo\") )\n",
    "\n",
    "agent10ui.ask(\"Why is there blood dripping form the sky?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Context/System prompt:** You are a sarcastic but helpful assistant.\n",
       "\n",
       "---\n",
       "**User[0]:** Why is there blood dripping form the sky?\n",
       "\n",
       "**Bot[0]:** I'm sorry, but as an AI language model, I don't have real-time information or the capability to observe the current situation. If blood is dripping from the sky in real life, it would be highly unusual and possibly indicate a dangerous or extraordinary event that requires immediate attention from relevant authorities. It is recommended to alert local emergency services or authorities in such cases.\n",
       "\n",
       "\n",
       "\n",
       "---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent10ui.show_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvAG6TLJ7du2",
    "outputId": "c647f2db-c559-4fa5-f763-4d3240f69c17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model', str), ('temperature', float)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Param:\n",
    "    type: T.Literal[\"int\", \"str\", \"float\"] = None\n",
    "    default: int | float | str = None\n",
    "    lo: int | float | str = None\n",
    "    hi: int | float | str = None\n",
    "    opts: list = None\n",
    "    desc: str = ''\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OpenAIParamsDescription:\n",
    "    model = Param(\n",
    "        type=str,\n",
    "        default=\"gpt-3.5-turbo\",\n",
    "        opts=(\n",
    "            \"gpt-3.5-turbo\",\n",
    "            \"gpt-3.5-turbo-0301\",\n",
    "            \"gpt-4-32k\",\n",
    "            \"gpt-4-32k-0314\"\n",
    "        ),\n",
    "    )\n",
    "    temperature = Param(\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        lo=0.0, hi=2.0,\n",
    "        desc=(\n",
    "            \"What sampling temperature to use, between 0 and 2. \"\n",
    "            \"Higher values like 0.8 will make the output more random, while \"\n",
    "            \"lower values like 0.2 will make it more focused and deterministic.\"\n",
    "            \"\\n\\n\"\n",
    "            \"We generally recommend altering this or top_p but not both.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    def make_dataclass(self):\n",
    "        fields = [\n",
    "            (k, getattr(self, k).type)\n",
    "            for k in dir(self)\n",
    "            if type(getattr(self, k)) == Param and k[0] != \"_\"\n",
    "        ]\n",
    "        return fields\n",
    "\n",
    "\n",
    "params = OpenAIParamsDescription()\n",
    "params.make_dataclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wsqsRoplFSXz",
    "outputId": "270533e6-5448-4289-d915-417688c0a25a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': Field(name='type',type=typing.Literal['int', 'str', 'float'],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x11132cf50>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " 'default': Field(name='default',type=int | float | str,default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x11132cf50>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " 'lo': Field(name='lo',type=int | float | str,default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x11132cf50>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " 'hi': Field(name='hi',type=int | float | str,default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x11132cf50>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " 'opts': Field(name='opts',type=<class 'list'>,default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x11132cf50>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " 'desc': Field(name='desc',type=<class 'str'>,default='',default_factory=<dataclasses._MISSING_TYPE object at 0x11132cf50>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Param.__dataclass_fields__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "-8HRkmXzlzQI",
    "outputId": "f81ddef1-9ac0-4cb1-d6f0-c52081fa23cd"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm not sure, but I'm sure he'd have some choice words for me."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import vertexai\n",
    "import vertexai.language_models as vxlm\n",
    "\n",
    "\n",
    "class VertexAIAgent(Agent):\n",
    "    conversation: Conversation\n",
    "    params: dict\n",
    "\n",
    "    _model: vxlm.ChatModel\n",
    "    _chat_session: vxlm.ChatSession = None\n",
    "\n",
    "    def __init__(self, conversation, **params):\n",
    "        self.conversation = conversation\n",
    "        self._model = vxlm.ChatModel.from_pretrained(params.pop(\"model\"))\n",
    "        self.params = params\n",
    "\n",
    "    def ask(\n",
    "        self, text: str, *, skip_history=False, **params\n",
    "    ) -> str:\n",
    "        new_messages = deepcopy(self.conversation.messages)\n",
    "        new_messages.append(Message(\"user\", text))\n",
    "\n",
    "        chat_session = self._get_chat_session()\n",
    "\n",
    "        send_message_args = {\n",
    "            **self.params,\n",
    "            **params,\n",
    "        }\n",
    "        self.last_res_ = chat_session.send_message(text, **send_message_args)\n",
    "\n",
    "        out = self.last_res_.text\n",
    "        assert out is not None\n",
    "\n",
    "        if not skip_history:\n",
    "            new_messages.append(Message(\"bot\", out))\n",
    "            self.conversation.messages = new_messages\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _get_chat_session(self):\n",
    "        if self._chat_session is None:\n",
    "            self._chat_session = self._model.start_chat(\n",
    "                context=self.conversation.context_prompt,\n",
    "                examples=[\n",
    "                    vxlm.InputOutputTextPair(input_text=e[0], output_text=e[1])\n",
    "                    for e in self.conversation.examples\n",
    "                ] if self.conversation.examples else None,\n",
    "                message_history=self._make_api_messages(self.conversation.messages)\n",
    "                    if self.conversation.messages else None,\n",
    "                **self.params,\n",
    "            )\n",
    "        return self._chat_session\n",
    "\n",
    "    def _make_api_messages(self, messages: list[Message]) -> list[dict]:\n",
    "        return [vars(m) for m in messages]\n",
    "\n",
    "\n",
    "conv = Conversation(\n",
    "    context_prompt=\"You are a sarcastic but helpful assistant.\",\n",
    "    messages=[]\n",
    ")\n",
    "agent20ui = AgentNbUI( VertexAIAgent(deepcopy(conv), model=\"chat-bison@001\") )\n",
    "\n",
    "agent20ui.ask(\"What would Hitler think of you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113
    },
    "id": "Pf61OY0vLlGu",
    "outputId": "10c52224-443c-4ef4-9d13-d378e4623770"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Context/System prompt:** You are a sarcastic but helpful assistant.\n",
       "\n",
       "---\n",
       "**User[0]:** What would Hitler think of you?\n",
       "\n",
       "**Bot[0]:** I'm not sure, but I'm sure he'd have some choice words for me.\n",
       "\n",
       "\n",
       "\n",
       "---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent20ui.show_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSG8KKYkfWzx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYDnDTkWShQY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qg0iR7uCccvw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "green-agent",
   "language": "python",
   "name": "green-agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
