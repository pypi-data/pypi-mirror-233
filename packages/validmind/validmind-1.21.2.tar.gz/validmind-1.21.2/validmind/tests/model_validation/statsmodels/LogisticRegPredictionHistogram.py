# Copyright Â© 2023 ValidMind Inc. All rights reserved.

from dataclasses import dataclass

import numpy as np
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from validmind.vm_models import Figure, Metric


@dataclass
class LogisticRegPredictionHistogram(Metric):
    """
    **Purpose**: The Logistic Regression Prediction Histogram is designed to generate two overlaid histograms for each
    of the training and test datasets, to evaluate and visualize the Probability of Default (PD) for positive and
    negative classes. This serves to analyze the performance of a logistic regression model, especially one related to
    credit risk prediction.

    **Test Mechanism**: The operation of this test involves the extraction of the target column from the train and test
    datasets, followed by the calculation of probabilities using the model's predict function. These probabilities are
    added as a new column to the training and testing dataframes. The histograms are generated by extracting subsets
    from these dataframes according to the target column and actual classes they represent (0 or 1 for binary
    classification scenarios), with different opacities set for better visualization. The four histograms (two for
    training data, two for test data) are overlaid on two subplot frames (one for training, one for testing), and the
    result is returned as a plotly graph object.

    **Signs of High Risk**: The possibility of high risk or failure in the model's performance arises if the histograms
    show significant discrepancies between the training and testing data, or between the positive and negative classes.
    These discrepancies could indicate overfitting or potential bias in the model. Additionally, if the probabilities
    are distributed unevenly, it may suggest that the model is not accurately predicting outcomes.

    **Strengths**: This test offers numerous strengths. First, it provides a visual representation of the PD generated
    by a machine learning model, which helps analyze and understand the model's behavior. Second, it can assess both
    the training and testing datasets, adding layers of validation to the model. Third, it highlights the disparity
    between multiple classes, offering potential insights into class imbalance or data skewness. Lastly, in the context
    of credit risk prediction, it is especially beneficial for visualizing the spread of risk across different classes.

    **Limitations**: Despite its strengths, the Logistic Regression Prediction Histogram has a few limitations. First,
    it is specifically tailored for binary classification scenarios, where the target variable only has two classes -
    making it inappropriate for multi-class classification tasks. Second, it is applicable mostly for logistic
    regression models and might not be as effective or accurate for testing other model types. Finally, while it gives
    a good visual representation of the model's PD predictions, it does not provide a quantifiable measure or score for
    model performance.
    """

    name = "logistic_reg_prediction_histogram"
    required_inputs = ["model"]
    metadata = {
        "task_types": ["classification"],
        "tags": ["tabular_data", "visualization", "credit_risk", "logistic_regression"],
    }

    default_params = {"title": "Histogram of Predictive Probabilities"}

    @staticmethod
    def compute_probabilities(model, X):
        """
        Predict probabilities and add PD as a new column in X
        """
        probabilities = model.predict(X)
        pd_series = probabilities

        # If X is a numpy array, convert it to DataFrame
        if isinstance(X, np.ndarray):
            X = pd.DataFrame(X)

        X["probabilities"] = pd_series
        return X

    @staticmethod
    def plot_prob_histogram(df_train, df_test, pd_col, target_col, title):
        train_0 = df_train[df_train[target_col] == 0][pd_col]
        train_1 = df_train[df_train[target_col] == 1][pd_col]
        test_0 = df_test[df_test[target_col] == 0][pd_col]
        test_1 = df_test[df_test[target_col] == 1][pd_col]

        fig = make_subplots(rows=1, cols=2, subplot_titles=("Train Data", "Test Data"))

        trace_train_0 = go.Histogram(
            x=train_0, opacity=0.75, name=f"Train {target_col} = 0"
        )
        trace_train_1 = go.Histogram(
            x=train_1, opacity=0.75, name=f"Train {target_col} = 1"
        )
        trace_test_0 = go.Histogram(
            x=test_0, opacity=0.75, name=f"Test {target_col} = 0"
        )
        trace_test_1 = go.Histogram(
            x=test_1, opacity=0.75, name=f"Test {target_col} = 1"
        )

        fig.add_trace(trace_train_0, row=1, col=1)
        fig.add_trace(trace_train_1, row=1, col=1)
        fig.add_trace(trace_test_0, row=1, col=2)
        fig.add_trace(trace_test_1, row=1, col=2)

        fig.update_layout(barmode="overlay", title_text=title)

        return fig

    def run(self):
        model = self.model[0] if isinstance(self.model, list) else self.model

        target_column = model.train_ds.target_column
        title = self.params["title"]

        # Create a copy of training and testing dataframes
        df_train = model.train_ds._df.copy()
        df_test = model.test_ds._df.copy()

        # Drop target_column to create feature dataframes
        X_train = df_train.drop(columns=[target_column])
        X_test = df_test.drop(columns=[target_column])

        # Subset only target_column to create target dataframes
        y_train = df_train[[target_column]]
        y_test = df_test[[target_column]]

        X_train = self.compute_probabilities(model, X_train)
        X_test = self.compute_probabilities(model, X_test)

        df_train = pd.concat([X_train, y_train], axis=1)
        df_test = pd.concat([X_test, y_test], axis=1)

        fig = self.plot_prob_histogram(
            df_train, df_test, "probabilities", target_column, title
        )

        return self.cache_results(
            metric_value={
                "prob_histogram": {
                    "train_probs": list(X_train["probabilities"]),
                    "test_probs": list(X_test["probabilities"]),
                },
            },
            figures=[
                Figure(
                    for_object=self,
                    key="prob_histogram",
                    figure=fig,
                )
            ],
        )
