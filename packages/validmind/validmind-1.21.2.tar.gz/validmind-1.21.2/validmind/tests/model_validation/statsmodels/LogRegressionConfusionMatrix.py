# Copyright Â© 2023 ValidMind Inc. All rights reserved.

from dataclasses import dataclass

import numpy as np
import plotly.figure_factory as ff
from sklearn import metrics

from validmind.vm_models import Figure, Metric


@dataclass
class LogRegressionConfusionMatrix(Metric):
    """
    **Purpose**: The Logistic Regression Confusion Matrix metric is designed to evaluate the performance of a logistic
    regression classification model by accounting for true positives, true negatives, false positives, and false
    negatives. By using this confusion matrix, one can visually determine how effectively the model is distinguishing
    between correct and incorrect classifications. The metric is particularly useful in scenarios where predictions are
    generated by thresholding probabilities.

    **Test Mechanism**: This metric makes use of Python's `sklearn.metrics.confusion_matrix` function to generate the
    confusion matrix. Initially, the model's predicted probabilities are adjusted to binary predictions using a
    provided cut-off threshold, by default set to 0.5. Afterwards, the confusion matrix is constructed using the actual
    and predicted classes. The confusion matrix is designed in such a way that predicted class labels form the x-axis
    and actual class labels form the y-axis. Each cell of the matrix contains the count of true positives, true
    negatives, false positives and false negatives respectively.

    **Signs of High Risk**: High risk or indications of model failure would be suggested by large numbers of false
    positives and false negatives. This would suggest that the model is incorrectly classifying a substantial number of
    instances. Also, if the true positive and true negative counts are significantly lower than expected, it can be
    seen as a high-risk indication.

    **Strengths**: The Logistic Regression Confusion Matrix is a simple and intuitive way to understand model
    performance. It not only provides a clear picture of how many instances are correctly and incorrectly classified,
    but also gives a breakdown of the types of errors. By adjusting the cut-off threshold parameter, users can explore
    trade-offs between precision (minimizing false positives) and recall (minimizing false negatives), making it
    flexible and adaptable for various prediction scenarios.

    **Limitations**: Despite its simplicity and clarity, the confusion matrix has its limitations. For imbalanced
    datasets, confusion matrix might give misleading results. For instance, a model might have a high accuracy by
    predicting majority classes, yet perform poorly on minority classes. It also does not give any insight into the
    severity of the mistakes and the cost trade-off between different kinds of misclassifications. Lastly, the choice
    of the cut-off threshold can significantly impact the interpretation, and an improperly chosen threshold may lead
    to flawed conclusions.
    """

    name = "log_regression_confusion_matrix"
    required_inputs = ["model"]
    metadata = {
        "task_types": ["classification"],
        "tags": ["visualization", "model_performance", "logistic_regression"],
    }

    default_params = {
        "cut_off_threshold": 0.5,  # Add a cut_off_threshold parameter
    }

    def run(self):
        cut_off_threshold = self.default_parameters["cut_off_threshold"]

        # Extract the actual model
        model = self.model[0] if isinstance(self.model, list) else self.model

        y_true = np.array(model.test_ds.y)
        y_labels = np.unique(y_true)
        y_labels.sort()

        y_pred_prob = model.predict(model.test_ds.x)
        y_pred = np.where(y_pred_prob > cut_off_threshold, 1, 0)
        y_true = y_true.astype(y_pred.dtype)

        cm = metrics.confusion_matrix(y_true, y_pred, labels=y_labels)
        tn, fp, fn, tp = cm.ravel()

        # Custom text to display on the heatmap cells
        text = [
            [
                f"<b>True Negatives (TN)</b><br />{tn}",
                f"<b>False Positives (FP)</b><br />{fp}",
            ],
            [
                f"<b>False Negatives (FN)</b><br />{fn}",
                f"<b>True Positives (TP)</b><br />{tp}",
            ],
        ]

        fig = ff.create_annotated_heatmap(
            [[tn, fp], [fn, tp]],
            x=[0, 1],
            y=[0, 1],
            colorscale="Blues",
            annotation_text=text,
        )
        # Reverse the xaxis so that 1 is on the left
        fig["layout"]["xaxis"]["autorange"] = "reversed"

        fig["data"][0][
            "hovertemplate"
        ] = "True Label:%{y}<br>Predicted Label:%{x}<br>Count:%{z}<extra></extra>"

        fig.update_layout(
            xaxis=dict(title="Predicted label", constrain="domain"),
            yaxis=dict(title="True label", scaleanchor="x", scaleratio=1),
            autosize=False,
            width=600,
            height=600,
        )

        return self.cache_results(
            metric_value={
                "tn": tn,
                "fp": fp,
                "fn": fn,
                "tp": tp,
            },
            figures=[
                Figure(
                    for_object=self,
                    key="confusion_matrix",
                    figure=fig,
                )
            ],
        )
